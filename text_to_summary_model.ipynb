{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Running on: ', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicion de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loader object\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, max_input_len):\n",
    "        self.data_df = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = max_input_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.data_df.iloc[idx]['texts']\n",
    "        topic = self.data_df.iloc[idx]['topic']\n",
    "\n",
    "        # Truncate the review if it exceeds the maximum input length\n",
    "        if len(review) > self.max_input_len:\n",
    "            review = review[:self.max_input_len]\n",
    "\n",
    "        input_ids = self.tokenizer.encode(review, max_length=self.max_input_len, pad_to_max_length=True, truncation=True)\n",
    "        output_ids = self.tokenizer.encode(topic, max_length=16, pad_to_max_length=True, truncation=True)\n",
    "        return input_ids, output_ids\n",
    "\n",
    "###############################\n",
    "###############################\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separar las secuencias de entrada y salida del lote\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    \n",
    "    # Convertir las listas de Python a tensores de PyTorch y enviarlos a la GPU\n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    output_ids = torch.tensor(output_ids).to(device)\n",
    "\n",
    "    return input_ids, output_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos y preprocesarlos\n",
    "data_df = pd.read_excel('/content/customer_reviews_data.xlsx')\n",
    "data_df.dropna(subset = 'topic', inplace = True)\n",
    "\n",
    "label = 'improvement'\n",
    "\n",
    "reviews_df = data_df[(data_df.label_raw == label) ].reset_index(drop=True)[['RespondentID', 'texts', 'topic', 'subtopic']]\n",
    "\n",
    "reviews_df.texts    = reviews_df.texts.astype(str)\n",
    "reviews_df.topic    = reviews_df.topic.astype(str)\n",
    "reviews_df.subtopic = reviews_df.subtopic.astype(str)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_df, test_df = train_test_split(reviews_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(  drop=True, inplace=True)\n",
    "test_df.reset_index( drop=True, inplace=True)\n",
    "\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer - Loand pre trained model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\n",
    "\n",
    "max_input_len = 512\n",
    "topic_sel = 'topic'\n",
    "\n",
    "# Crear un objeto DataLoader para el conjunto de datos de entreno\n",
    "train_dataset = MyDataset(train_df, tokenizer, max_input_len=max_input_len)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Crear un objeto DataLoader para el conjunto de datos de validacion\n",
    "val_dataset = MyDataset(val_df, tokenizer, max_input_len=max_input_len)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Crear un objeto DataLoader para el conjunto de datos de prueba\n",
    "test_dataset = MyDataset(test_df, tokenizer, max_input_len=max_input_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Entrenar el modelo con el DataLoader\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        input_ids, output_ids = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids=input_ids, labels=output_ids).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      val_loss = 0\n",
    "      for batch in val_dataloader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        input_ids, output_ids = batch\n",
    "        loss = model(input_ids=input_ids, labels=output_ids).loss\n",
    "        val_loss += loss.item()\n",
    "      val_loss /= len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss / len(train_dataloader):.4f}, Val Loss={val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar t√≠tulos para el conjunto de datos de prueba\n",
    "model.eval()\n",
    "title_list = []\n",
    "for batch in test_dataloader:\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    input_ids = batch[0]\n",
    "    generated_titles = model.generate(input_ids, max_length=16, num_beams=4, no_repeat_ngram_size=2)\n",
    "    for i in range(len(input_ids)):\n",
    "        title = tokenizer.decode(generated_titles[i], skip_special_tokens=True)\n",
    "        title_list.append(title) \n",
    "\n",
    "# Guardar los resultados en el DataFrame de prueba\n",
    "test_df['topic_result'] = title_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_excel('../result_data.xlsx', index = False)\n",
    "model.save_pretrained(\"../t5_model_improvement\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
